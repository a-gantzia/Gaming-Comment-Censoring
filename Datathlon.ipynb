{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP team Data Challenge!!!\n",
    "Hello interns! This is your first day working at BigBlueSteam Co!\n",
    "\n",
    "Until 17.00 oâ€™clock you have to use your Steam Game Review Dataset to create a model that predicts from text whether the user recommends a game title or not.\n",
    "\n",
    "Until now our platform hosts reviews and game publishers accuse us of hosting comments that are often  unjustifiable and not based on evidence but on rage. Our company will use your model real-time so positive comments can be publicly available instantly. On the other hand, negative ones have to be detected in order to be reviewed before becoming public so we can avoid excessive language and unjustifiable accusations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vassilis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vassilis/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "import re\n",
    "from re import sub\n",
    "from PIL import Image\n",
    "import textwrap\n",
    "import urllib.request\n",
    "#from wordcloud import WordCloud, ImageColorGenerator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>user_review</th>\n",
       "      <th>user_suggestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Spooky's Jump Scare Mansion</td>\n",
       "      <td>2016.00</td>\n",
       "      <td>I'm scared and hearing creepy voices.  So I'll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spooky's Jump Scare Mansion</td>\n",
       "      <td>2016.00</td>\n",
       "      <td>Best game, more better than Sam Pepper's YouTu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spooky's Jump Scare Mansion</td>\n",
       "      <td>2016.00</td>\n",
       "      <td>A littly iffy on the controls, but once you kn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Spooky's Jump Scare Mansion</td>\n",
       "      <td>2015.00</td>\n",
       "      <td>Great game, fun and colorful and all that.A si...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Spooky's Jump Scare Mansion</td>\n",
       "      <td>2015.00</td>\n",
       "      <td>Not many games have the cute tag right next to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id                        title    year  \\\n",
       "0          1  Spooky's Jump Scare Mansion 2016.00   \n",
       "1          2  Spooky's Jump Scare Mansion 2016.00   \n",
       "2          3  Spooky's Jump Scare Mansion 2016.00   \n",
       "3          4  Spooky's Jump Scare Mansion 2015.00   \n",
       "4          5  Spooky's Jump Scare Mansion 2015.00   \n",
       "\n",
       "                                         user_review  user_suggestion  \n",
       "0  I'm scared and hearing creepy voices.  So I'll...                1  \n",
       "1  Best game, more better than Sam Pepper's YouTu...                1  \n",
       "2  A littly iffy on the controls, but once you kn...                1  \n",
       "3  Great game, fun and colorful and all that.A si...                1  \n",
       "4  Not many games have the cute tag right next to...                1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train = pd.read_csv('.\\\\data\\\\train_gr\\\\train.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>developer</th>\n",
       "      <th>publisher</th>\n",
       "      <th>tags</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spooky's Jump Scare Mansion</td>\n",
       "      <td>Lag Studios</td>\n",
       "      <td>Lag Studios</td>\n",
       "      <td>['Horror', 'Free to Play', 'Cute', 'First-Pers...</td>\n",
       "      <td>Can you survive 1000 rooms of cute terror? Or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sakura Clicker</td>\n",
       "      <td>Winged Cloud</td>\n",
       "      <td>Winged Cloud</td>\n",
       "      <td>['Nudity', 'Anime', 'Free to Play', 'Mature', ...</td>\n",
       "      <td>The latest entry in the Sakura series is more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WARMODE</td>\n",
       "      <td>WARTEAM</td>\n",
       "      <td>WARTEAM</td>\n",
       "      <td>['Early Access', 'Free to Play', 'FPS', 'Multi...</td>\n",
       "      <td>Free to play shooter about the confrontation o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fractured Space</td>\n",
       "      <td>Edge Case Games Ltd.</td>\n",
       "      <td>Edge Case Games Ltd.</td>\n",
       "      <td>['Space', 'Multiplayer', 'Free to Play', 'PvP'...</td>\n",
       "      <td>Take the helm of a gigantic capital ship and g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Counter-Strike: Global Offensive</td>\n",
       "      <td>Valve, Hidden Path Entertainment</td>\n",
       "      <td>Valve</td>\n",
       "      <td>['FPS', 'Multiplayer', 'Shooter', 'Action', 'T...</td>\n",
       "      <td>Counter-Strike: Global Offensive (CS: GO) expa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title                          developer  \\\n",
       "0       Spooky's Jump Scare Mansion                       Lag Studios    \n",
       "1                    Sakura Clicker                      Winged Cloud    \n",
       "2                           WARMODE                           WARTEAM    \n",
       "3                   Fractured Space              Edge Case Games Ltd.    \n",
       "4  Counter-Strike: Global Offensive  Valve, Hidden Path Entertainment    \n",
       "\n",
       "               publisher                                               tags  \\\n",
       "0           Lag Studios   ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
       "1          Winged Cloud   ['Nudity', 'Anime', 'Free to Play', 'Mature', ...   \n",
       "2               WARTEAM   ['Early Access', 'Free to Play', 'FPS', 'Multi...   \n",
       "3  Edge Case Games Ltd.   ['Space', 'Multiplayer', 'Free to Play', 'PvP'...   \n",
       "4                 Valve   ['FPS', 'Multiplayer', 'Shooter', 'Action', 'T...   \n",
       "\n",
       "                                            overview  \n",
       "0  Can you survive 1000 rooms of cute terror? Or ...  \n",
       "1  The latest entry in the Sakura series is more ...  \n",
       "2  Free to play shooter about the confrontation o...  \n",
       "3  Take the helm of a gigantic capital ship and g...  \n",
       "4  Counter-Strike: Global Offensive (CS: GO) expa...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games = pd.read_csv('game_overview.csv')\n",
    "games.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1603</td>\n",
       "      <td>Counter-Strike: Global Offensive</td>\n",
       "      <td>2015.00</td>\n",
       "      <td>Nice graphics, new maps, weapons and models. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1604</td>\n",
       "      <td>Counter-Strike: Global Offensive</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>I would not recommend getting into this at its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1605</td>\n",
       "      <td>Counter-Strike: Global Offensive</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>Edit 11/12/18I have tried playing CS:GO recent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1606</td>\n",
       "      <td>Counter-Strike: Global Offensive</td>\n",
       "      <td>2015.00</td>\n",
       "      <td>The game is great. But the community is the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1607</td>\n",
       "      <td>Counter-Strike: Global Offensive</td>\n",
       "      <td>2015.00</td>\n",
       "      <td>I thank TrulyRazor for buying this for me a lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id                             title    year  \\\n",
       "0       1603  Counter-Strike: Global Offensive 2015.00   \n",
       "1       1604  Counter-Strike: Global Offensive 2018.00   \n",
       "2       1605  Counter-Strike: Global Offensive 2018.00   \n",
       "3       1606  Counter-Strike: Global Offensive 2015.00   \n",
       "4       1607  Counter-Strike: Global Offensive 2015.00   \n",
       "\n",
       "                                         user_review  \n",
       "0  Nice graphics, new maps, weapons and models. B...  \n",
       "1  I would not recommend getting into this at its...  \n",
       "2  Edit 11/12/18I have tried playing CS:GO recent...  \n",
       "3  The game is great. But the community is the wo...  \n",
       "4  I thank TrulyRazor for buying this for me a lo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test = pd.read_csv('.\\\\data\\\\test_gr\\\\test.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Before anything else we need at some point all bad-language to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.    It would be a great idea to accompany your script with a spell checking mechanism. (Demonstrate  it on single reviews not all dataset it may too take looong!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\n",
      "  Downloading symspellpy-6.7.6-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.6 MB 850 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting editdistpy>=0.1.3\n",
      "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: editdistpy\n",
      "  Building wheel for editdistpy (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for editdistpy: filename=editdistpy-0.1.3-cp37-cp37m-macosx_10_9_x86_64.whl size=27169 sha256=10b5f8dbf1be3842e5d3f72b13ce7ad3e26c82b907cfa67ebf21c0f51a649159\n",
      "  Stored in directory: /Users/vassilis/Library/Caches/pip/wheels/d2/ba/f8/1cb9b9798794b9fd49081c7372160c3e6195acb2ea60d7cb9c\n",
      "Successfully built editdistpy\n",
      "Installing collected packages: editdistpy, symspellpy\n",
      "Successfully installed editdistpy-0.1.3 symspellpy-6.7.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -U symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog, 8, -34.491167981910635\n",
      "Spell Checking sÃ¼resi: 0.0002617835998535156\n",
      "I hated you u., 3, -23.575165850893608\n",
      "Spell Checking sÃ¼resi: 0.0007350444793701172\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "from symspellpy.symspellpy import SymSpell\n",
    "import time\n",
    "\n",
    "# Set max_dictionary_edit_distance to avoid spelling correction\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# a sentence without any spaces\n",
    "input_term_one = \"thequickbrownfoxjumpsoverthelazydog\"\n",
    "input_term_two = \"I hated youu.\"\n",
    "result = sym_spell.word_segmentation(input_term_one)\n",
    "start = time.time()\n",
    "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
    "                          result.log_prob_sum))\n",
    "end = time.time()\n",
    "print(\"Spell Checking sÃ¼resi:\",(end - start))\n",
    "start = time.time()\n",
    "result = sym_spell.word_segmentation(input_term_two)\n",
    "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
    "                          result.log_prob_sum))\n",
    "end = time.time()\n",
    "print(\"Spell Checking sÃ¼resi:\",(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the love he had dated for much of the past who couldn't read in six grade and inspired him, 9, 0\n",
      "Spell Checking sÃ¼resi: 0.016719818115234375\n",
      "i hate the book but i like it this girls, 5, 0\n",
      "Spell Checking sÃ¼resi: 0.0052661895751953125\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import time\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# lookup suggestions for multi-word input strings (supports compound\n",
    "# splitting & merging)\n",
    "input_term = (\"whereis th elove hehad dated forImuch of thepast who \"\n",
    "              \"couqdn'tread in sixtgrade and ins pired him\")\n",
    "# max edit distance per lookup (per single word, not per whole input string)\n",
    "start = time.time()\n",
    "suggestions = sym_spell.lookup_compound(input_term, max_edit_distance=2)\n",
    "# display suggestion term, edit distance, and term frequency\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Spell Checking sÃ¼resi:\",(end - start))\n",
    "input_term_two = (\"I hate thi boook but I like it this girll\")\n",
    "start = time.time()\n",
    "suggestions_two = sym_spell.lookup_compound(input_term_two,max_edit_distance=2)\n",
    "for suggestion in suggestions_two:\n",
    "    print(suggestion)\n",
    "end = time.time()\n",
    "print(\"Spell Checking sÃ¼resi:\",(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the love he HAd dated for much of thE Past who couldn't read in six grade and inspired him, 9, 0\n",
      "Spell Checking zamanÄ±: 0.030076026916503906\n",
      "I hate the book but I like it this girls, 3, 0\n",
      "Spell Checking zamanÄ±: 0.005163908004760742\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import time\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# lookup suggestions for multi-word input strings (supports compound\n",
    "# splitting & merging)\n",
    "input_term = (\"whereis th elove heHAd dated forImuch of thEPast who \"\n",
    "              \"couqdn'tread in sixtgrade and ins pired him\")\n",
    "# max edit distance per lookup (per single word, not per whole input string)\n",
    "start = time.time()\n",
    "suggestions = sym_spell.lookup_compound(input_term, max_edit_distance=2,\n",
    "                                        transfer_casing=True)\n",
    "# display suggestion term, edit distance, and term frequency\n",
    "\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion)\n",
    "end = time.time()\n",
    "print(\"Spell Checking zamanÄ±:\",(end - start))\n",
    "input_term_two = (\"I hate thi boook but I like it this girll\")\n",
    "start = time.time()\n",
    "# max edit distance per lookup (per single word, not per whole input string)\n",
    "suggestions_two = sym_spell.lookup_compound(input_term_two, max_edit_distance=2,\n",
    "                                        transfer_casing=True)\n",
    "for suggestion in suggestions_two:\n",
    "    print(suggestion)\n",
    "end = time.time()\n",
    "print(\"Spell Checking zamanÄ±:\",(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c1b0ef0ff397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mto_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# if you're impatient switch this flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m     51\u001b[0m     return util.load_model(\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The spell checker has been entirely ripped off of this script by Serg Lavrikov(rumbok):\n",
    "https://www.kaggle.com/rumbok/ridge-lb-0-41944\n",
    "\n",
    "do check it out, its a work of art.\n",
    "\n",
    "caveat: script consumes a lot of memory but is much faster than Norvig's spell checker (1 million times)\n",
    "http://blog.faroo.com/2015/03/24/fast-approximate-string-matching-with-large-edit-distances/\n",
    "\"\"\"\n",
    "\n",
    "import time \n",
    "import re, random\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "to_sample = False # if you're impatient switch this flag\n",
    "\n",
    "def spacy_tokenize(text):\n",
    "    return [token.text for token in nlp.tokenizer(text)]\n",
    "    \n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    This method has not been modified from the original.\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
    "    for x in range(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
    "        for y in range(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                    and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "\n",
    "class SymSpell:\n",
    "    def __init__(self, max_edit_distance=3, verbose=0):\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        # 0: top suggestion\n",
    "        # 1: all suggestions of smallest edit distance\n",
    "        # 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
    "\n",
    "        self.dictionary = {}\n",
    "        self.longest_word_length = 0\n",
    "\n",
    "    def get_deletes_list(self, w):\n",
    "        \"\"\"given a word, derive strings with up to max_edit_distance characters\n",
    "           deleted\"\"\"\n",
    "\n",
    "        deletes = []\n",
    "        queue = [w]\n",
    "        for d in range(self.max_edit_distance):\n",
    "            temp_queue = []\n",
    "            for word in queue:\n",
    "                if len(word) > 1:\n",
    "                    for c in range(len(word)):  # character index\n",
    "                        word_minus_c = word[:c] + word[c + 1:]\n",
    "                        if word_minus_c not in deletes:\n",
    "                            deletes.append(word_minus_c)\n",
    "                        if word_minus_c not in temp_queue:\n",
    "                            temp_queue.append(word_minus_c)\n",
    "            queue = temp_queue\n",
    "\n",
    "        return deletes\n",
    "\n",
    "    def create_dictionary_entry(self, w):\n",
    "        '''add word and its derived deletions to dictionary'''\n",
    "        # check if word is already in dictionary\n",
    "        # dictionary entries are in the form: (list of suggested corrections,\n",
    "        # frequency of word in corpus)\n",
    "        new_real_word_added = False\n",
    "        if w in self.dictionary:\n",
    "            # increment count of word in corpus\n",
    "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
    "        else:\n",
    "            self.dictionary[w] = ([], 1)\n",
    "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
    "\n",
    "        if self.dictionary[w][1] == 1:\n",
    "            # first appearance of word in corpus\n",
    "            # n.b. word may already be in dictionary as a derived word\n",
    "            # (deleting character from a real word)\n",
    "            # but counter of frequency of word in corpus is not incremented\n",
    "            # in those cases)\n",
    "            new_real_word_added = True\n",
    "            deletes = self.get_deletes_list(w)\n",
    "            for item in deletes:\n",
    "                if item in self.dictionary:\n",
    "                    # add (correct) word to delete's suggested correction list\n",
    "                    self.dictionary[item][0].append(w)\n",
    "                else:\n",
    "                    # note frequency of word in corpus is not incremented\n",
    "                    self.dictionary[item] = ([w], 0)\n",
    "\n",
    "        return new_real_word_added\n",
    "\n",
    "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        for line in arr:\n",
    "            # separate by words by non-alphabetical characters\n",
    "            words = re.findall(token_pattern, line.lower())\n",
    "            for word in words:\n",
    "                total_word_count += 1\n",
    "                if self.create_dictionary_entry(word):\n",
    "                    unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def create_dictionary(self, fname):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        with open(fname) as file:\n",
    "            for line in file:\n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', line.lower())\n",
    "                for word in words:\n",
    "                    total_word_count += 1\n",
    "                    if self.create_dictionary_entry(word):\n",
    "                        unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def get_suggestions(self, string, silent=False):\n",
    "        \"\"\"return list of suggested corrections for potentially incorrectly\n",
    "           spelled word\"\"\"\n",
    "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
    "            if not silent:\n",
    "                print(\"no items in dictionary within maximum edit distance\")\n",
    "            return []\n",
    "\n",
    "        suggest_dict = {}\n",
    "        min_suggest_len = float('inf')\n",
    "\n",
    "        queue = [string]\n",
    "        q_dictionary = {}  # items other than string that we've checked\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            q_item = queue[0]  # pop\n",
    "            queue = queue[1:]\n",
    "\n",
    "            # early exit\n",
    "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
    "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
    "                break\n",
    "\n",
    "            # process queue item\n",
    "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
    "                if self.dictionary[q_item][1] > 0:\n",
    "                    # word is in dictionary, and is a word from the corpus, and\n",
    "                    # not already in suggestion list so add to suggestion\n",
    "                    # dictionary, indexed by the word with value (frequency in\n",
    "                    # corpus, edit distance)\n",
    "                    # note q_items that are not the input string are shorter\n",
    "                    # than input string since only deletes are added (unless\n",
    "                    # manual dictionary corrections are added)\n",
    "                    assert len(string) >= len(q_item)\n",
    "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
    "                                            len(string) - len(q_item))\n",
    "                    # early exit\n",
    "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
    "                        break\n",
    "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
    "                        min_suggest_len = len(string) - len(q_item)\n",
    "\n",
    "                # the suggested corrections for q_item as stored in\n",
    "                # dictionary (whether or not q_item itself is a valid word\n",
    "                # or merely a delete) can be valid corrections\n",
    "                for sc_item in self.dictionary[q_item][0]:\n",
    "                    if sc_item not in suggest_dict:\n",
    "\n",
    "                        # compute edit distance\n",
    "                        # suggested items should always be longer\n",
    "                        # (unless manual corrections are added)\n",
    "                        assert len(sc_item) > len(q_item)\n",
    "\n",
    "                        # q_items that are not input should be shorter\n",
    "                        # than original string\n",
    "                        # (unless manual corrections added)\n",
    "                        assert len(q_item) <= len(string)\n",
    "\n",
    "                        if len(q_item) == len(string):\n",
    "                            assert q_item == string\n",
    "                            item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                        # item in suggestions list should not be the same as\n",
    "                        # the string itself\n",
    "                        assert sc_item != string\n",
    "\n",
    "                        # calculate edit distance using, for example,\n",
    "                        # Damerau-Levenshtein distance\n",
    "                        item_dist = dameraulevenshtein(sc_item, string)\n",
    "\n",
    "                        # do not add words with greater edit distance if\n",
    "                        # verbose setting not on\n",
    "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
    "                            pass\n",
    "                        elif item_dist <= self.max_edit_distance:\n",
    "                            assert sc_item in self.dictionary  # should already be in dictionary if in suggestion list\n",
    "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
    "                            if item_dist < min_suggest_len:\n",
    "                                min_suggest_len = item_dist\n",
    "\n",
    "                        # depending on order words are processed, some words\n",
    "                        # with different edit distances may be entered into\n",
    "                        # suggestions; trim suggestion dictionary if verbose\n",
    "                        # setting not on\n",
    "                        if self.verbose < 2:\n",
    "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
    "\n",
    "            # now generate deletes (e.g. a substring of string or of a delete)\n",
    "            # from the queue item\n",
    "            # as additional items to check -- add to end of queue\n",
    "            assert len(string) >= len(q_item)\n",
    "\n",
    "            # do not add words with greater edit distance if verbose setting\n",
    "            # is not on\n",
    "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
    "                pass\n",
    "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
    "                for c in range(len(q_item)):  # character index\n",
    "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
    "                    if word_minus_c not in q_dictionary:\n",
    "                        queue.append(word_minus_c)\n",
    "                        q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "\n",
    "        # queue is now empty: convert suggestions in dictionary to\n",
    "        # list for output\n",
    "        if not silent and self.verbose != 0:\n",
    "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
    "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "\n",
    "        # output option 1\n",
    "        # sort results by ascending order of edit distance and descending\n",
    "        # order of frequency\n",
    "        #     and return list of suggested word corrections only:\n",
    "        # return sorted(suggest_dict, key = lambda x:\n",
    "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "        # output option 2\n",
    "        # return list of suggestions with (correction,\n",
    "        #                                  (frequency in corpus, edit distance)):\n",
    "        as_list = suggest_dict.items()\n",
    "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
    "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
    "\n",
    "        if self.verbose == 0:\n",
    "            return outlist[0]\n",
    "        else:\n",
    "            return outlist\n",
    "\n",
    "        '''\n",
    "        Option 1:\n",
    "        ['file', 'five', 'fire', 'fine', ...]\n",
    "        Option 2:\n",
    "        [('file', (5, 0)),\n",
    "         ('five', (67, 1)),\n",
    "         ('fire', (54, 1)),\n",
    "         ('fine', (17, 1))...]  \n",
    "        '''\n",
    "\n",
    "    def best_word(self, s, silent=False):\n",
    "        try:\n",
    "            return self.get_suggestions(s, silent)[0]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def spell_corrector(word_list, words_d) -> str:\n",
    "    result_list = []\n",
    "    for word in word_list:\n",
    "        if word not in words_d:\n",
    "            suggestion = ss.best_word(word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                result_list.append(suggestion)\n",
    "        else:\n",
    "            result_list.append(word)\n",
    "            \n",
    "    return \" \".join(result_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # build symspell tree \n",
    "    ss = SymSpell(max_edit_distance=2)\n",
    "    \n",
    "    # fetch list of bad words\n",
    "    with open('../input/bad-bad-words/bad-words.csv') as bf:\n",
    "        bad_words = bf.readlines()\n",
    "    bad_words = [word.strip() for word in bad_words]    \n",
    "    \n",
    "    # fetch english words dictionary\n",
    "    with open('../input/479k-english-words/english_words_479k.txt') as f:\n",
    "        words = f.readlines()\n",
    "    eng_words = [word.strip() for word in words]\n",
    "    \n",
    "    # Print some examples\n",
    "    print(eng_words[:5])\n",
    "    print(bad_words[:5])\n",
    "\n",
    "    print('Total english words: {}'.format(len(eng_words)))\n",
    "    print('Total bad words: {}'.format(len(bad_words)))\n",
    "    \n",
    "    print('create symspell dict...')\n",
    " \n",
    "    if to_sample:\n",
    "        # sampling from list for kernel runtime\n",
    "        sample_idxs = random.sample(range(len(eng_words)), 100)\n",
    "        eng_words = [eng_words[i] for i in sorted(sample_idxs)] + \\\n",
    "        'to infinity and beyond'.split() # make sure our sample misspell is in there\n",
    "    \n",
    "    all_words_list = list(set(bad_words + eng_words))\n",
    "    silence = ss.create_dictionary_from_arr(all_words_list, token_pattern=r'.+')\n",
    "    \n",
    "    # create a dictionary of rightly spelled words for lookup\n",
    "    words_dict = {k: 0 for k in all_words_list}\n",
    "    \n",
    "    sample_text = 'to infifity and byond'\n",
    "    tokens = spacy_tokenize(sample_text)\n",
    "        \n",
    "    \n",
    "    print('run spell checker...')\n",
    "    print()\n",
    "    print('original text: ' + sample_text)\n",
    "    print()\n",
    "    start = time.time()\n",
    "    correct_text = spell_corrector(tokens, words_dict)\n",
    "    print('corrected text: ' + correct_text)\n",
    "    end = time.time()\n",
    "    print('Spell Checker sÃ¼resi:',(end - start))\n",
    "        \n",
    "    sample_text_two = 'I likeee planetes anddd I  like somebody.'\n",
    "    tokens = spacy_tokenize(sample_text_two)\n",
    "    print('Two run spell checker...')\n",
    "    print()\n",
    "    print('original text: ' + sample_text)\n",
    "    print()\n",
    "    start = time.time()\n",
    "    correct_text = spell_corrector(tokens, words_dict)\n",
    "    print('corrected text: ' + correct_text)\n",
    "    end = time.time()\n",
    "    print('Spell Checker sÃ¼resi:',(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Try to capture the comments with the most intense feelings of joy or anger.    Provide us with a model and script to run you classification from command line (you could keep a portion of data as a new csv to try it as external data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Last but not least, any analysis on the reviews would be valuable and highly appreciated!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdfdf\n",
    "fdfd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
