{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP team Data Challenge!!!\n",
    "Hello interns! This is your first day working at BigBlueSteam Co!\n",
    "\n",
    "Until 17.00 oâ€™clock you have to use your Steam Game Review Dataset to create a model that predicts from text whether the user recommends a game title or not.\n",
    "\n",
    "Until now our platform hosts reviews and game publishers accuse us of hosting comments that are often  unjustifiable and not based on evidence but on rage. Our company will use your model real-time so positive comments can be publicly available instantly. On the other hand, negative ones have to be detected in order to be reviewed before becoming public so we can avoid excessive language and unjustifiable accusations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install better-profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "import re\n",
    "from re import sub\n",
    "from PIL import Image\n",
    "import textwrap\n",
    "import urllib.request\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('.\\\\data\\\\train_gr\\\\train.csv')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.read_csv('.\\\\data\\\\train_gr\\\\game_overview.csv')\n",
    "games.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('.\\\\data\\\\test_gr\\\\test.csv')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for performing a summarising, preliminary EDA\n",
    "def initial_eda(df):\n",
    "    \"\"\" Prints the dataFrames's column names, data types, \n",
    "    number of distinct values, number of missing values\n",
    "    @param df: pandas DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        total_na = df.isna().sum().sum()\n",
    "        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n",
    "        print(\"Total NA Values : %d \" % (total_na))\n",
    "        print(\"%35s %20s   %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n",
    "        col_name = df.columns\n",
    "        dtyp = df.dtypes\n",
    "        uniq = df.nunique()\n",
    "        na_val = df.isna().sum()\n",
    "        for i in range(len(df.columns)):\n",
    "            print(\"%35s %20s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n",
    "    else:\n",
    "        print(\"Expect a DataFrame but got a %15s\" % (type(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_eda(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_eda(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_eda(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "def clean_text(comment):\n",
    "    return ''.join(re.sub(r\"(@[A-Za-z0-9]+)|(http\\S+)|(\\$[A-Za-z0-9]+)|([0-9]+)\",\"\",comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(comment):# unrolls hastags and also removes symbols\n",
    "    for remove in map(lambda r: re.compile(re.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                                                                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                     \"!\", \"?\", \".\", \"'\",\n",
    "                                                                     \"--\", \"---\", \"#\"]):\n",
    "        comment.replace(remove, \"\", inplace=True)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords1 = pd.read_csv('badwords.txt', header=None)\n",
    "badwords2 = pd.read_csv('bad-words.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = badwords1.append(badwords2)\n",
    "bad.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list \n",
    "                                              or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Before anything else we need at some point all bad-language to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df[\"clean_text\"]=df['user_review'].apply(lambda row: remove_stopwords(row))\n",
    "    df[\"clean_text\"]=df['clean_text'].apply(lambda row: clean_text(row))\n",
    "    remove_special_chars(df.clean_text)\n",
    "    df[\"badwordcount\"] = df['user_review'].apply(\n",
    "        lambda comment: sum(comment.count(str(w)) for w in bad))\n",
    "    df['num_words'] = df['user_review'].apply(\n",
    "            lambda comment: len(comment.split()))\n",
    "    df['num_chars'] = df['user_review'].apply(len)\n",
    "    df[\"normword_badwords\"] = df[\"badwordcount\"]/df['num_words']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocessing(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['profanity_flag'] = train['clean_text'].apply(lambda x: profanity.contains_profanity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['profanity_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.    It would be a great idea to accompany your script with a spell checking mechanism. (Demonstrate  it on single reviews not all dataset it may too take looong!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Try to capture the comments with the most intense feelings of joy or anger.    Provide us with a model and script to run you classification from command line (you could keep a portion of data as a new csv to try it as external data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Last but not least, any analysis on the reviews would be valuable and highly appreciated!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angel ................"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
